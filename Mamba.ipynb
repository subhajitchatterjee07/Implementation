{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5EgbyRNSftKh95fIsoodb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhajitchatterjee07/Implementation/blob/main/Mamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning about Mamba (LSTM with Selective state space)"
      ],
      "metadata": {
        "id": "iLGWFLRvCkia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mamba is a recent model and is thought to be a better model compared to Transformers. Just to remember the existing models:\n",
        "\n",
        "1. Transformers: have an attention mechanism where any part of the sequence can dynamically interact with any other. The ones with causal attention are good at handling individual elements of a sequence.\n",
        "Problem with them is that they come with a significant computational and memory cost, scaling with the square of the sequence length(L^2).\n",
        "2. Recurrent Neural Networks: RNNs update a hidden state sequentially, considering only the current input and the last hidden state. This approach allows them to potentially handle infinite sequence lengths and that too with constant memory requirements. Yet, this simplicity might cause problems like limiting their ability to remember long term dependencies. Additionally, backpropagation through time (BPTT) in RNNs can be memory intensive and may suffer from vanishing/exploding gradients, despite innovations like LSTM.\n",
        "3. State-Space Models: These models have given promising results. They have both, the ability to capture long range dependencies more effectively than RNNs, and are also more memory-efficient than transformers.\n",
        "\n",
        "\n",
        "## What Mamba does:\n",
        "1. Mamba builds upon the concept of SSMs but it leverages selective state spaces to enable more efficient and effective capture of relevant information across long sequences.\n",
        "2. Linear Time complexity: Mamba operates in linear time w.r.t. seq. length. This property mmakes it suitable for tasks involving long sequences.\n",
        "\n",
        "\n",
        "## Selective State Spaces\n",
        "Mamba has a slightly different approach than the traditional state space models, making it more adaptible and flexible, somewhat akin to LSTMs. However it retains the efficient computation trait of state space models, enabling it to perform the forward passes of an entire sequence in one sweep- a feature present in transformers.\n",
        "\n",
        "## Training and inference with Mamba\n",
        "During training, Mamba behaves like transformers, processing the entire sequence in one go. This approach contrasts with LSTMs, where forward pass must be computed step by step, even if all inputs are known. In inference, Mamba's behaviour aligns nmore with traditional recurrence models, offering efficient processing of sequences.\n",
        "\n",
        "## Limitations of prior Models\n",
        "A key limitation of prior SSMs is their rigid, input-invariant structure. Typically, these models employ a set of fixed parameters(let's call them A and B) for the entire sequence. This structure is even more restrictive than models like LSTMs, where the transformation of signals can depend on the previous hidden state and the input.\n",
        "\n",
        "## The Mamba approach\n",
        "What Mamba introduces is a shift in how the transition to the next hidden state is computed. In Mamba's architecture, the transition can be dependent on the current input. This approach strikes a balance between the fixed computational backbone of traditional SSMs and the input-dependent dynamism of RNNs.\n",
        "### Key components:\n",
        "1. Fixed Backbone:\n",
        "The transition from one hidden state(defined by A matrix), allowing for precomputation across the sequence.\n",
        "2. Input-dependent transformation: The way the input influences the next hidden state(defined by B matrix) is dependent on the current input, not on the previous hidden state. This input dependency allows for more flexibility compared to traditional SSMs.\n",
        "\n",
        "### Overcoming computational challenges\n",
        "To address the commputational demands of the approach, mamba utilizes a hardware-aware algorithm. The algorithm performs computations recurrently using a scen operation instead of convolution, making it highly efficient for GPUs. This efficiency is crucial for maintaining high performance despite the algorithmic complexity introduced by the input dependent transitions.\n",
        "\n",
        "### Mamba vs Selective State Space\n",
        "It is important to clarify that Mamba and selectively state space models are not synonymous. Mamba is an implementation that uses the concept of selective state spaces.  This distinction is crucial because it highlights Mamba's unique contribution: adapting the SSM framework to be more flexible and input-responsive while retaining computational efficiency.\n",
        "\n",
        "\n",
        "Mamba promises to be the bridge between highly flexible but computationally efficient Transformers and efficient but rigid traditional SSMs. This balance could potentially unlock new capabilities in processing long sequences across various domains, from NLPs to genomic sequencing.\n",
        "\n",
        "### GPU Memory: SRAM and HBM\n",
        "GPUs contain 2 primary types of memory: High bandwidth memory (HBM) and Static Random Access Memory(SRAM). HBM, though high bandwidth, has a relatively slower access time compared to much faster but smaller SRAM. Understanding this, Mamba strategically uses SRAM for rapid access during matrix multiplications, which form the crux of its computations.\n",
        "\n",
        "### Overcoming Data movement bottlenecks:\n",
        "The primary bottleneck in computation is often not the calculations themselves but the movement of various data between the memory types. Mamba addresses this by significantly reducing the need to transfer large amounts of data. It does so by executing the critical parts of the algorithm, like discretization and recurrence computations, directly in SRAM, thus reducing latency.\n",
        "### The fused selective scan layer:\n",
        "Mamba introduces a fused selective scan layer, which brings its memory requirements on par with  optimized transformer implementations using Flash attention. This layer is crucial for maintaining efficiency, especially when dealing with input-dependent elements in the model.\n",
        "### Efficient computation with prefix sums/parallel scans\n",
        "Mamba utilizes prefix sums or parallel scans for efficient computation. Unlike convolutions, which require a constant kernel, prefix sums can handle the varying elements introduced by Mamba's input-dependence. This method is essential for computing the cumulative multiplications of the matrices at different timesteps.\n",
        "### Experimental Results and Scaling\n",
        "Mamba has shown promising results in language modeling and DNA sequencing, areas where long sequences are prevalent. Its scalability and efficiency, even at longer sequence lengths, make it a strong candidate for a general sequence model backbone."
      ],
      "metadata": {
        "id": "hn0aVomOCxAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBwbhDvBKE6s",
        "outputId": "312c89f2-b13e-4f29-c99e-31fbf35d86c2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip show zip_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWK5kRpWkYyG",
        "outputId": "bd58d980-1b2b-4e9d-ec54-b0e83d043e91"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: zip-files\n",
            "Version: 0.4.1\n",
            "Summary: Command line utilities for creating zip files\n",
            "Home-page: https://github.com/goerz/zip_files\n",
            "Author: Michael Goerz\n",
            "Author-email: mail@michaelgoerz.net\n",
            "License: BSD license\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: click\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install zip_files"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yv_0XmbkgRm",
        "outputId": "54078fba-32bc-4778-e0ea-06f9e0656111"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zip_files in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from zip_files) (8.1.7)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import zipfile"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z71v0EDXkg2-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1\n",
        "#import the libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import functional as F\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "import math\n",
        "import os\n",
        "import urllib.request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "3D6RO5VwCtxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab6972f-e32b-4e41-f2eb-11cfb2a468d5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7db075f9f340>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_MAMBA = 1\n",
        "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "BtPtn7LYwCyy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up flags and hyperparameters and flags.\n",
        "Also configured the GPU for training."
      ],
      "metadata": {
        "id": "T0-cDq7_yV5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#user_defined_hyperparameters\n",
        "d_model = 8 #dimensions of model\n",
        "state_size = 128 #example state size\n",
        "seq_len = 100 #example sequence length\n",
        "batch_size = 256 #example batch size\n",
        "last_batch_size = 81 #only for the very last batch of the dataset\n",
        "current_batch_size = batch_size\n",
        "different_batch_size = False\n",
        "h_new = None\n",
        "temp_buffer = None"
      ],
      "metadata": {
        "id": "PC0WO9MMyVAa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the S6 module\n",
        "The s6 class represents a sophisticated component within the Mamba architecture, responsible for processing input sequences through a series of linear transformations and a discretization process. It plays a critical role in capturing the temporal dynamics of sequences, a key aspect of sequence modelling tasks such as language modelling. The class showcases advanced techniques such as tensor operations and custom discretization methods to handle the complex requirements of sequence data."
      ],
      "metadata": {
        "id": "iEIYlXxJ0sg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discretization function is defined based on the Mamba paper's description using ZOH on page 28, in section : Mechanics on selective SSMs.\n",
        "See also \"Zero order hold discretization\" maths proof inside https://studywolf.wordpress.com/tag/zero-order-hold/\n",
        "\n",
        "\n",
        "Here is an explanation for the mathematical rationale for the formulation of Δt used in Mamba:\n",
        "\n",
        "The key idea is that Δt contros the discretization rate of the continous SSM dynamics. By making Δt input-independent, it introduces selectivity into discrete transition matrices.\n",
        "Specifically, in Mamba they parametrize Δt as:\n",
        "Δt = τΔ(Parameter + sΔ(xt))"
      ],
      "metadata": {
        "id": "UzpzKxn08pM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an explanation of the mathematical rationale for the formulation of Δt used in Mamba:\n",
        "  The key idea is that Δt controls the discretization rate of the continous SSM dynamics. By making Δt input dependent, it introduces selectivity into discrete transition matrices. Specifically, in Mamba they parametrize Δt as:\n",
        "   Δt = τΔ(Parameter + sΔ(xt))\n",
        "   Where :\n",
        "   - Parameter is a learned scalar parameter that controls the baseline discretization rate\n",
        "   - sΔ(xt) is a projection that makes Δt input-dependent by computing a value based on xt\n",
        "   - τΔ(x) = softplus(x) transforms the result to be positive through the softplus nonlinearity.\n",
        "   The rationale for this formulation is:\n",
        "   - Parameter provides a reasonable default discretization rate.\n",
        "   - sΔ(xt) injects input-dependence through projection\n",
        "   - softplus ensures Δt is positive as required to be a valid timestep\n",
        "   - the projection sΔ allows the model to learn modulate Δ\n",
        "   - This modulation creates selectivity in how rapidly or slowly the states update\n",
        "   So the summary, the learned input-dependent projection allows Δt, and thus the discrete dynamics, to become selective. The softplus and scalar parameter provide useful inductive biases on the top of this flexibility.\n",
        "   The end result is discrete transition matrices that are selective on the input, enabling powerful sequence meodelling capabilities."
      ],
      "metadata": {
        "id": "z5yx8UaFElWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class S6(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, state_size, device):\n",
        "        super(S6, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, d_model, device=device)\n",
        "        self.fc2 = nn.Linear(d_model, state_size, device=device)\n",
        "        self.fc3 = nn.Linear(d_model, state_size, device=device)\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.state_size = state_size\n",
        "\n",
        "        #self.A = nn.Parameter(torch.ones(d_model, state_size, device=device))\n",
        "        self.A = nn.Parameter(F.normalize(torch.ones(d_model, state_size, device=device), p=2, dim=-1))\n",
        "        nn.init.xavier_uniform_(self.A)\n",
        "\n",
        "        self.B = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
        "        self.C = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
        "\n",
        "        self.delta = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
        "        self.dA = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
        "        self.dB = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
        "\n",
        "        # h should have dimensions [batch_size, seq_len, d_model, state_size]\n",
        "        self.h = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
        "        self.y = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
        "\n",
        "\n",
        "    def discretization(self):\n",
        "        # discretization function is defined based on the MAMBA paper's description using ZOH on page 28\n",
        "        # in Section C : Mechanics on Selective SSMs\n",
        "        # See also \"Zero-order hold discretization\" maths proof inside https://studywolf.wordpress.com/tag/zero-order-hold/\n",
        "        \"\"\"\n",
        "        Here is an explanation of the mathematical rationale for the formulation of Δt used in Mamba:\n",
        "        The key idea is that Δt controls the discretization rate of the continuous SSM dynamics. By making Δt input-dependent, it introduces selectivity into the discrete transition matrices.\n",
        "        Specifically, in Mamba they parameterize Δt as:\n",
        "        Δt = τΔ(Parameter + sΔ(xt))\n",
        "        Where:\n",
        "        - Parameter is a learned scalar parameter that controls the baseline discretization rate\n",
        "        - sΔ(xt) is a projection that makes Δt input-dependent by computing a value based on xt\n",
        "        - τΔ(x) = softplus(x) transforms the result to be positive through the softplus nonlinearity\n",
        "        The rationale for this formulation is:\n",
        "        - Parameter provides a reasonable default discretization rate\n",
        "        - sΔ(xt) injects input-dependence through the projection\n",
        "        - softplus ensures Δt is positive as required to be a valid timestep\n",
        "        - The projection sΔ allows the model to learn to modulate Δt based on the input xt\n",
        "        - This modulation creates selectivity in how rapidly or slowly the states update\n",
        "        So in summary, the learned input-dependent projection allows Δt, and thus the discrete dynamics, to become selective. The softplus and scalar parameter provide useful inductive biases on top of this flexibility.\n",
        "        The end result is discrete transition matrices that are selective on the input, enabling powerful sequence modeling capabilities.\n",
        "        Credit: Claude2 AI chatbot\n",
        "        \"\"\"\n",
        "\n",
        "        # inverse() only supports square matrix\n",
        "        #dB = torch.matmul(torch.inverse(A * delta), torch.matmul(dA - torch.eye(A.shape[0]), B))\n",
        "        self.dB = torch.einsum(\"bld,bln->bldn\", self.delta, self.B)\n",
        "\n",
        "        # https://github.com/state-spaces/mamba/blob/0131c1e94a46fc9f70bcfc9d57962963bb2f0b9e/mamba_ssm/modules/mamba_simple.py#L240\n",
        "        #dA = torch.matrix_exp(A * delta)  # matrix_exp() only supports square matrix\n",
        "        self.dA = torch.exp(torch.einsum(\"bld,dn->bldn\", self.delta, self.A))\n",
        "        #print(f\"self.dA.shape = {self.dA.shape}\")\n",
        "        #print(f\"self.dA.requires_grad = {self.dA.requires_grad}\")\n",
        "\n",
        "        return self.dA, self.dB\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Refer to Algorithm 2 in the MAMBA paper\n",
        "        self.B = self.fc2(x)\n",
        "        self.C = self.fc3(x)\n",
        "        self.delta = F.softplus(self.fc1(x))\n",
        "\n",
        "        # Uses ZOH as in MAMBA, Hungry Hippo still uses bilinear transform for discretization\n",
        "        self.discretization()\n",
        "\n",
        "        if DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:  # this will trigger in-place runtime error if without using `h_new`\n",
        "\n",
        "            global current_batch_size\n",
        "            current_batch_size = x.shape[0]\n",
        "\n",
        "            if self.h.shape[0] != current_batch_size:\n",
        "                #print(\"Adjusting h_new for the different batch size of input data `x`\")\n",
        "                different_batch_size = True\n",
        "\n",
        "                # Resize self.h to match the current batch size\n",
        "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h[:current_batch_size, ...]) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
        "\n",
        "            else:\n",
        "                different_batch_size = False\n",
        "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
        "\n",
        "            # y needs to have a shape of [batch_size, seq_len, d_model]\n",
        "            self.y = torch.einsum('bln,bldn->bld', self.C, h_new)\n",
        "\n",
        "            # Update self.h with the detached state of h_new\n",
        "            # Only do this if retaining gradients for self.h is not necessary for backprop\n",
        "            # Otherwise, store h_new in a temporary list and update self.h after the loop\n",
        "            global temp_buffer\n",
        "            temp_buffer = h_new.detach().clone() if not self.h.requires_grad else h_new.clone()\n",
        "\n",
        "            return self.y\n",
        "\n",
        "        else:  # this will not trigger in-place runtime error\n",
        "            # h should have dimensions [batch_size, seq_len, d_model, state_size]\n",
        "            h = torch.zeros(x.size(0), self.seq_len, self.d_model, self.state_size, device=x.device)\n",
        "            y = torch.zeros_like(x)\n",
        "\n",
        "            h =  torch.einsum('bldn,bldn->bldn', self.dA, h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
        "\n",
        "            # y needs to have a shape of [batch_size, seq_len, d_model]\n",
        "            y = torch.einsum('bln,bldn->bld', self.C, h)\n",
        "\n",
        "            return y\n",
        "\n"
      ],
      "metadata": {
        "id": "JYTVkr31EgI9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, state_size, device):\n",
        "        super(MambaBlock, self).__init__()\n",
        "\n",
        "        self.inp_proj = nn.Linear(d_model, 2*d_model, device=device)\n",
        "        self.out_proj = nn.Linear(2*d_model, d_model, device=device)\n",
        "\n",
        "        # For residual skip connection\n",
        "        self.D = nn.Linear(d_model, 2*d_model, device=device)\n",
        "\n",
        "        # Set _no_weight_decay attribute on bias\n",
        "        self.out_proj.bias._no_weight_decay = True\n",
        "\n",
        "        # Initialize bias to a small constant value\n",
        "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
        "\n",
        "        self.S6 = S6(seq_len, 2*d_model, state_size, device)\n",
        "\n",
        "        # Add 1D convolution with kernel size 3\n",
        "        self.conv = nn.Conv1d(seq_len, seq_len, kernel_size=3, padding=1, device=device)\n",
        "\n",
        "        # Add linear layer for conv output\n",
        "        self.conv_linear = nn.Linear(2*d_model, 2*d_model, device=device)\n",
        "\n",
        "        # rmsnorm\n",
        "        self.norm = RMSNorm(d_model, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x_proj.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
        "        x_conv.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
        "        x_conv_act.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
        "        \"\"\"\n",
        "        # Refer to Figure 3 in the MAMBA paper\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        x_proj = self.inp_proj(x)\n",
        "        #print(f\"x_proj.shape = {x_proj.shape}\")\n",
        "\n",
        "        # Add 1D convolution with kernel size 3\n",
        "        x_conv = self.conv(x_proj)\n",
        "        #print(f\"x_conv.shape = {x_conv.shape}\")\n",
        "\n",
        "        x_conv_act = F.silu(x_conv)\n",
        "        #print(f\"x_conv_act.shape = {x_conv_act.shape}\")\n",
        "\n",
        "        # Add linear layer for conv output\n",
        "        x_conv_out = self.conv_linear(x_conv_act)\n",
        "        #print(f\"x_conv_out.shape = {x_conv_out.shape}\")\n",
        "\n",
        "        x_ssm = self.S6(x_conv_out)\n",
        "        x_act = F.silu(x_ssm)  # Swish activation can be implemented as x * sigmoid(x)\n",
        "        #print(f\"x_act.shape = {x_act.shape}\")\n",
        "\n",
        "        # residual skip connection with nonlinearity introduced by multiplication\n",
        "        x_residual = F.silu(self.D(x))\n",
        "        #print(f\"x_residual.shape = {x_residual.shape}\")\n",
        "        x_combined = x_act * x_residual\n",
        "        #print(f\"x_combined.shape = {x_combined.shape}\")\n",
        "\n",
        "        x_out = self.out_proj(x_combined)\n",
        "        #print(f\"x_out.shape = {x_out.shape}\")\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "e3NG0DaehgE7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mamba(nn.Module):\n",
        "    def __init__(self, seq_len, d_model, state_size, device):\n",
        "        super(Mamba, self).__init__()\n",
        "        self.mamba_block1 = MambaBlock(seq_len, d_model, state_size, device)\n",
        "        self.mamba_block2 = MambaBlock(seq_len, d_model, state_size, device)\n",
        "        self.mamba_block3 = MambaBlock(seq_len, d_model, state_size, device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mamba_block1(x)\n",
        "        x = self.mamba_block2(x)\n",
        "        x = self.mamba_block3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fbUXgOPriKDO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5,\n",
        "                 device: str ='cuda'):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Emc7FHUXiqBv"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(batch_size, seq_len, d_model, device=device)\n",
        "# Create the Mamba model\n",
        "mamba = Mamba(seq_len, d_model, state_size, device)\n",
        "\n",
        "# rmsnorm\n",
        "norm = RMSNorm(d_model)\n",
        "x = norm(x)\n",
        "\n",
        "# Forward pass\n",
        "test_output = mamba(x)\n",
        "print(f\"test_output.shape = {test_output.shape}\")  # Should be [batch_size, seq_len, d_model]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X989qi2NiteV",
        "outputId": "b3c7a754-b61b-4530-97d7-f6d1ba6d7776"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_output.shape = torch.Size([256, 100, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Enwiki8Dataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.data.items()}\n",
        "        return item"
      ],
      "metadata": {
        "id": "r1sx_I9aiwYx"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for padding\n",
        "def pad_sequences_3d(sequences, max_len=None, pad_value=0):\n",
        "    # Assuming sequences is a tensor of shape (batch_size, seq_len, feature_size)\n",
        "    batch_size, seq_len, feature_size = sequences.shape\n",
        "\n",
        "    if max_len is None:\n",
        "        max_len = seq_len + 1\n",
        "\n",
        "\n",
        "    # Initialize padded_sequences with the pad_value\n",
        "    padded_sequences = torch.full((batch_size, max_len, feature_size), fill_value=pad_value, dtype=sequences.dtype, device=sequences.device)\n",
        "    # Pad each sequence to the max_len\n",
        "    padded_sequences[:, :seq_len, :] = sequences\n",
        "\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "A2tdHNdQjfwf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, data_loader, optimizer, criterion, device, max_grad_norm=1.0, DEBUGGING_IS_ON=False):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_data = batch['input_ids'].clone().to(device)\n",
        "        attention_mask = batch['attention_mask'].clone().to(device)\n",
        "\n",
        "        # In most sequence modeling tasks, like language modeling, the target should be the next token\n",
        "        # in the sequence rather than the input token itself.\n",
        "        # This is because the model's goal is to predict the next word given the previous words.\n",
        "        # Shift the input data by one position to get the target, so that each target token\n",
        "        # is the next token following the input token.\n",
        "        target = input_data[:, 1:]\n",
        "        input_data = input_data[:, :-1]\n",
        "\n",
        "        # Pad all the sequences in the batch:\n",
        "        input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
        "        target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
        "\n",
        "        if USE_MAMBA:\n",
        "            output = model(input_data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # Clip gradients: gradients are modified in place\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        for name, param in model.named_parameters():\n",
        "           if 'out_proj.bias' not in name:\n",
        "               # clip weights but not bias for out_proj\n",
        "               torch.nn.utils.clip_grad_norm_(param, max_norm=max_grad_norm)\n",
        "\n",
        "        if DEBUGGING_IS_ON:\n",
        "            for name, parameter in model.named_parameters():\n",
        "                if parameter.grad is not None:\n",
        "                    print(f\"{name} gradient: {parameter.grad.data.norm(2)}\")\n",
        "                else:\n",
        "                    print(f\"{name} has no gradient\")\n",
        "\n",
        "        if USE_MAMBA and DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:\n",
        "            model.S6.h[:current_batch_size, ...].copy_(temp_buffer)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "YySlE5yNjiTD"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_data = batch['input_ids'].clone().detach().to(device)\n",
        "            attention_mask = batch['attention_mask'].clone().detach().to(device)\n",
        "\n",
        "            # In most sequence modeling tasks, like language modeling, the target should be the next token\n",
        "            # in the sequence rather than the input token itself.\n",
        "            # This is because the model's goal is to predict the next word given the previous words.\n",
        "            # Shift the input data by one position to get the target, so that each target token\n",
        "            # is the next token following the input token.\n",
        "            target = input_data[:, 1:]\n",
        "            input_data = input_data[:, :-1]\n",
        "\n",
        "            # Pad all the sequences in the batch:\n",
        "            input_data = pad_sequences_3d(input_data, pad_value=tokenizer.pad_token_id)\n",
        "            target = pad_sequences_3d(target, max_len=input_data.size(1), pad_value=tokenizer.pad_token_id)\n",
        "\n",
        "            if USE_MAMBA:\n",
        "                output = model(input_data)\n",
        "                loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "O4maPi5YjlAP"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(loss):\n",
        "    return math.exp(loss)"
      ],
      "metadata": {
        "id": "P3T0UT4hjoMX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_enwiki8_dataset():\n",
        "    print(f\"Download and extract enwiki8 data\")\n",
        "    url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
        "    urllib.request.urlretrieve(url, \"enwik8.zip\")\n",
        "\n",
        "    with ZipFile(\"enwik8.zip\") as f:\n",
        "        data = f.read(\"enwik8\").decode(\"utf-8\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "KCCqGcq1jzJ2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode the dataset\n",
        "def encode_dataset(tokenizer, text_data):\n",
        "    def batch_encode(tokenizer, text_data, batch_size=1000):\n",
        "        # Tokenize in batches\n",
        "        batched_input_ids = []\n",
        "        for i in range(0, len(text_data), batch_size):\n",
        "            batch = text_data[i:i+batch_size]\n",
        "            inputs = tokenizer(batch, add_special_tokens=True, truncation=True,\n",
        "                               padding='max_length', max_length=seq_len,\n",
        "                               return_tensors='pt')\n",
        "            batched_input_ids.append(inputs['input_ids'])\n",
        "        return torch.cat(batched_input_ids)\n",
        "\n",
        "    # Assuming enwiki8_data is a list of sentences\n",
        "    input_ids = batch_encode(tokenizer, enwiki8_data)\n",
        "\n",
        "    # vocab_size is the number of unique tokens in the tokenizer's vocabulary\n",
        "    global vocab_size\n",
        "    vocab_size = len(tokenizer.vocab)  # Note that for some tokenizers, we might access the vocab directly\n",
        "    print(f\"vocab_size = {vocab_size}\")\n",
        "\n",
        "    # Create an embedding layer\n",
        "    # embedding_dim is the size of the embedding vectors (MAMBA model's D)\n",
        "    embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "\n",
        "    # Pass `input_ids` through the embedding layer\n",
        "    # This will change `input_ids` from shape [B, L] to [B, L, D]\n",
        "    #encoded_input = embedding_layer(input_ids)   ## this eats memory, so use batched_embedding_calls instead\n",
        "    def batch_embedding_calls(input_ids, embedding_layer, batch_size=256):\n",
        "        # Check if input_ids is already a tensor, if not convert it\n",
        "        if not isinstance(input_ids, torch.Tensor):\n",
        "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "        # Calculate the number of batches needed\n",
        "        num_batches = math.ceil(input_ids.size(0) / batch_size)\n",
        "\n",
        "        # List to hold the output embeddings\n",
        "        output_embeddings = []\n",
        "\n",
        "        # Process each batch\n",
        "        for i in range(num_batches):\n",
        "            # Calculate start and end indices for the current batch\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "\n",
        "            # Get the batch\n",
        "            input_id_batch = input_ids[start_idx:end_idx]\n",
        "\n",
        "            # Call the embedding layer\n",
        "            with torch.no_grad():  # No need gradients for this operation\n",
        "                batch_embeddings = embedding_layer(input_id_batch)\n",
        "\n",
        "            # Append the result to the list\n",
        "            output_embeddings.append(batch_embeddings)\n",
        "\n",
        "        # Concatenate the embeddings from each batch into a single tensor\n",
        "        all_embeddings = torch.cat(output_embeddings, dim=0)\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "    # `input_ids` is a list or tensor of the input IDs and `embedding_layer` is model's embedding layer\n",
        "    if USE_MAMBA:\n",
        "        # Set `batch_size` to a value that works for memory constraints\n",
        "        encoded_inputs = batch_embedding_calls(input_ids, embedding_layer, batch_size=1).float()\n",
        "\n",
        "    attention_mask = (input_ids != tokenizer.pad_token_id).type(input_ids.dtype)\n",
        "\n",
        "    return encoded_inputs, attention_mask"
      ],
      "metadata": {
        "id": "_R2CfbKVj1aC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "syfedz8YkEu5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming encoded_inputs is a preprocessed tensor of shape [num_samples, seq_len, d_model]\n",
        "encoded_inputs_file = 'encoded_inputs_mamba.pt'\n",
        "\n",
        "\n",
        "if os.path.exists(encoded_inputs_file):\n",
        "    print(\"Loading pre-tokenized data...\")\n",
        "    encoded_inputs = torch.load(encoded_inputs_file)\n",
        "else:\n",
        "    print(\"Tokenizing raw data...\")\n",
        "    enwiki8_data = load_enwiki8_dataset()\n",
        "    encoded_inputs, attention_mask = encode_dataset(tokenizer, enwiki8_data)\n",
        "    torch.save(encoded_inputs, encoded_inputs_file)\n",
        "    print(f\"finished tokenizing data\")\n",
        "\n",
        "\n",
        "# Combine into a single dictionary\n",
        "data = {\n",
        "    'input_ids': encoded_inputs,\n",
        "    'attention_mask': attention_mask\n",
        "}\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "total_size = len(data['input_ids'])\n",
        "train_size = int(total_size * 0.8)\n",
        "\n",
        "train_data = {key: val[:train_size] for key, val in data.items()}\n",
        "val_data = {key: val[train_size:] for key, val in data.items()}\n",
        "\n",
        "train_dataset = Enwiki8Dataset(train_data)\n",
        "val_dataset = Enwiki8Dataset(val_data)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "model = Mamba(seq_len, d_model, state_size, device).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 25  # Number of epochs to train for\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n",
        "    train_loss = train(model, tokenizer, train_loader, optimizer, criterion, device, max_grad_norm=10.0, DEBUGGING_IS_ON=False)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    val_perplexity = calculate_perplexity(val_loss)\n",
        "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Perplexity: {val_perplexity:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys-W7eQbkHVe",
        "outputId": "9444990f-4ef7-4283-e422-dc8023725086"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing raw data...\n",
            "Download and extract enwiki8 data\n",
            "vocab_size = 30522\n",
            "finished tokenizing data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 1/25 [01:20<32:02, 80.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: -4.4533, Validation Loss: -5.7294, Validation Perplexity: 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 2/25 [02:39<30:36, 79.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Training Loss: -4.4611, Validation Loss: -5.7470, Validation Perplexity: 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 3/25 [03:59<29:12, 79.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Training Loss: -4.5059, Validation Loss: -5.8712, Validation Perplexity: 0.0028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 4/25 [05:18<27:50, 79.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Training Loss: -8.0943, Validation Loss: -21.4852, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 5/25 [06:38<26:33, 79.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Training Loss: -49.7272, Validation Loss: -82.6938, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 6/25 [07:58<25:16, 79.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Training Loss: -139.8954, Validation Loss: -197.8762, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 7/25 [09:18<23:56, 79.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Training Loss: -307.8996, Validation Loss: -409.9386, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 8/25 [10:39<22:41, 80.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Training Loss: -607.3140, Validation Loss: -776.2809, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 9/25 [11:59<21:25, 80.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Training Loss: -1120.4014, Validation Loss: -1399.4222, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 10/25 [13:20<20:07, 80.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Training Loss: -1985.1196, Validation Loss: -2438.0631, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 11/25 [14:41<18:45, 80.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Training Loss: -3396.0594, Validation Loss: -4121.0314, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 12/25 [16:00<17:23, 80.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Training Loss: -5671.3287, Validation Loss: -6808.0584, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 13/25 [17:20<16:02, 80.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Training Loss: -9285.9885, Validation Loss: -11079.8674, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 14/25 [18:41<14:42, 80.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Training Loss: -15003.7683, Validation Loss: -17821.7826, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 15/25 [20:01<13:21, 80.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Training Loss: -23983.7659, Validation Loss: -28335.5159, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 16/25 [21:21<12:01, 80.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Training Loss: -37907.8720, Validation Loss: -44572.5927, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 17/25 [22:41<10:41, 80.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17, Training Loss: -59144.7030, Validation Loss: -69052.8578, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 18/25 [24:01<09:20, 80.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18, Training Loss: -90554.4735, Validation Loss: -104812.1774, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 19/25 [25:22<08:01, 80.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19, Training Loss: -136133.1153, Validation Loss: -156402.3925, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 20/25 [26:44<06:43, 80.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Training Loss: -200947.0880, Validation Loss: -228849.0625, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 21/25 [28:06<05:25, 81.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21, Training Loss: -291866.5336, Validation Loss: -330356.0304, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 22/25 [29:29<04:05, 81.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22, Training Loss: -417620.5732, Validation Loss: -469720.2248, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 23/25 [30:52<02:44, 82.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23, Training Loss: -590254.9548, Validation Loss: -661680.9423, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 24/25 [32:14<01:22, 82.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24, Training Loss: -826559.9042, Validation Loss: -922239.3726, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [33:35<00:00, 80.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, Training Loss: -1147515.0503, Validation Loss: -1274747.3814, Validation Perplexity: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "glJTYM7Eufr3"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}